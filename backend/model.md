# CaseCut Legal Chatbot â€” Model Documentation

## ğŸ§  Models Used

### 1. LLM â€” Llama 3.3 70B Versatile (via Groq)

| Property | Value |
|----------|-------|
| **Model** | `llama-3.3-70b-versatile` |
| **Provider** | Groq (free tier: 14,400 req/day) |
| **Fallback** | Google Gemini 2.0 Flash |
| **Use case** | Legal Q&A, case analysis, mode-based responses |
| **Max tokens** | 800 (output) |
| **Temperature** | 0.3 (low creativity, high accuracy) |

**Why chosen:**
- 70B parameters give strong legal reasoning capability
- Groq provides extremely fast inference (< 1s for most queries)
- Free tier is generous enough for development and moderate production use
- Gemini 2.0 Flash as fallback ensures 99.9% uptime

### 2. Embedding Model â€” all-MiniLM-L6-v2

| Property | Value |
|----------|-------|
| **Model** | `sentence-transformers/all-MiniLM-L6-v2` |
| **Dimensions** | 384 |
| **Max sequence** | 256 tokens |
| **Size** | ~80 MB |
| **Use case** | Query & document embedding for Qdrant vector search |

**Why chosen:**
- Extremely fast inference (CPU-friendly)
- 384-dim vectors are compact â†’ lower Qdrant storage
- Good semantic understanding for legal text similarity
- No GPU required â€” runs on any machine

### 3. Summarizer â€” Flan-T5 Small (Optional Local)

| Property | Value |
|----------|-------|
| **Model** | `google/flan-t5-small` |
| **Size** | ~300 MB |
| **Use case** | Local document summarization (optional) |
| **Fallback** | Cloud LLM (Groq/Gemini) |

**Why chosen:**
- Can run entirely offline for privacy-sensitive documents
- Small footprint, no GPU needed
- Falls back to cloud LLM when transformer libraries aren't installed

---

## ğŸ’» Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **CPU** | 2 cores | 4+ cores |
| **RAM** | 4 GB | 8 GB |
| **GPU** | Not required | CUDA GPU (speeds up embedding) |
| **Disk** | 2 GB | 5 GB (for models + data) |
| **Network** | Required (for Groq/Gemini/Qdrant Cloud) | â€” |

> **Note:** All heavy LLM inference happens on Groq/Gemini cloud. The local machine only runs embedding (MiniLM) and optional Flan-T5 summarization.

---

## ğŸ“ Folder Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app + startup logging
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py         # Centralized singletons (Qdrant, LLM, embedder)
â”‚   â”‚   â”œâ”€â”€ logic.py          # RAG pipeline + intelligent PDF processing
â”‚   â”‚   â”œâ”€â”€ prompts.py        # Role-aware prompt templates (4 modes)
â”‚   â”‚   â””â”€â”€ history.py        # User session history
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ embeddings.py     # Embedding + Qdrant upload utilities
â”‚   â”‚   â”œâ”€â”€ ranker.py         # Feature-based case re-ranking
â”‚   â”‚   â””â”€â”€ summarizer.py     # Optional local Flan-T5 summarizer
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ query.py          # /query endpoint (RAG search)
â”‚   â”‚   â”œâ”€â”€ upload.py         # /upload + /summarize endpoints
â”‚   â”‚   â””â”€â”€ feedback.py       # /feedback endpoint
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ parser.py         # PDF/TXT parsing + metadata extraction
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Input PDFs/TXT files
â”‚   â””â”€â”€ processed/            # Parsed JSON metadata
â”œâ”€â”€ model.md                  # â† This file
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                      # API keys (not committed)
```

---

## ğŸ”„ RAG Pipeline

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embed Query â”‚  â† all-MiniLM-L6-v2 (384-dim)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant Search   â”‚  â† top-k*2 vectors with optional topic filter
â”‚ (Vector DB)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Re-Rank Cases    â”‚  â† Multi-feature scoring:
â”‚                  â”‚     semantic (40%) + IPC match (20%)
â”‚                  â”‚     + topic (15%) + court authority (15%)
â”‚                  â”‚     + recency (10%)
â”‚                  â”‚     + role-aware bias adjustments
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build Prompt     â”‚  â† Mode-specific system prompt
â”‚ (Role-Aware)     â”‚     + structured output instructions
â”‚                  â”‚     + retrieved case context
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Generation   â”‚  â† Groq (Llama 3.3 70B) â†’ Gemini fallback
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  Structured Response
  (summary + case citations)
```

---

## ğŸ“„ Intelligent PDF Processing

```
PDF Upload
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract Text    â”‚  â† PyMuPDF (fitz)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  text length > 4000 chars?
       â”‚
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚ YES     â”‚ NO
  â–¼         â–¼
Extract    Send directly
Key Points  to LLM
  â”‚
  â–¼
Condensed text
  â”‚
  â–¼
Mode-aware LLM Summarization
```

---

## ğŸš€ Future Scalability Plan

### Short-term
- [ ] Add **BGE-large** or **E5-large** embeddings for better legal retrieval
- [ ] Implement **conversation memory** (multi-turn RAG)
- [ ] Add **citation linking** to actual judgment PDFs

### Medium-term
- [ ] Fine-tune embedding model on Indian legal corpus
- [ ] Add **hybrid search** (dense + sparse/BM25) in Qdrant
- [ ] Implement **streaming responses** via SSE
- [ ] Add **multi-language support** (Hindi legal texts)

### Long-term
- [ ] Fine-tune a legal-specific LLM on Indian judgments
- [ ] Implement **knowledge graph** for case relationships
- [ ] Add **automated case outcome prediction**
- [ ] Deploy on-premise for law firms (fully offline mode)
